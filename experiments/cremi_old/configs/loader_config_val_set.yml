# Config for pretraining AND training.

# Specify the names of the datasets
dataset_names:
  - A
  - B
  - C

# Specify how the data needs to be sliced before feeding to the network.
# We use a 3D sliding window over the dataset to extract patches, which
# are then fed to the network as batches.
slicing_config:
  # Sliding window size
  window_size:
    A: [25, 478, 478]
    B: [25, 478, 478]
    C: [25, 478, 478]
#    A: [25, 956, 956]
#    B: [25, 956, 956]
#    C: [25, 956, 956]
  # Sliding window stride
  stride:
    A: [1, 222, 222]
    B: [1, 222, 222]
    C: [1, 222, 222]
#    A: [1, 430, 430]
#    B: [1, 430, 430]
#    C: [1, 430, 430]
#    A: [1, 444, 444]
#    B: [1, 444, 444]
#    C: [1, 444, 444]
  # Example:
  #   window_size = [1, 512, 512], downsampling_ratio = [1, 2, 2] ==>
  #   slice shape = [1, 256, 256]
  downsampling_ratio:
    A: [1, 1, 1]
    B: [1, 1, 1]
    C: [1, 1, 1]
  # Reflect padding on the loaded volume. Follows numpy.pad semantics.
  # IMPORTANT no reflect padding along z !
  padding:
    A: [[5, 0], [50, 50], [50, 50]]
    B: [[5, 0], [50, 50], [50, 50]]
    C: [[5, 0], [50, 50], [50, 50]]
  # Data slice to iterate over.
  data_slice:
    A: '85:, :, :'
    B: '85:, :, :'
    C: '85:, :, :'



# Specify paths to volumes
volume_config:
  # Raw data
  raw:
    path:
      A: '/export/home/abailoni/datasets/cremi/SOA_affinities/sampleA_train.h5'
      B: '/export/home/abailoni/datasets/cremi/SOA_affinities/sampleB_train.h5'
      C: '/export/home/abailoni/datasets/cremi/SOA_affinities/sampleC_train.h5'
    path_in_h5_dataset:
      A: 'raw'
      B: 'raw'
      C: 'raw'
    ignore_slice_list:
      B: [15, 16, 44, 45, 77]
      C: [14, 74, 86]
#      C: [19, 79, 91]
    # Optionally, we specify training precision
    dtype: float32
  GT:
    dtype: int32
    path:
      A: '/export/home/abailoni/datasets/cremi/SOA_affinities/sampleA_train.h5'
      B: '/export/home/abailoni/datasets/cremi/SOA_affinities/sampleB_train.h5'
      C: '/export/home/abailoni/datasets/cremi/SOA_affinities/sampleC_train.h5'
    path_in_h5_dataset: {A: 'segmentations/groundtruth_fixed', B: 'segmentations/groundtruth_fixed', C: 'segmentations/groundtruth_fixed'}



# Configuration for the master dataset.
master_config:
  # We might need order 0 interpolation if we have segmentation in there somewhere.
  elastic_transform:
    alpha: 2000.
    sigma: 50.
    order: 0
  # we crop to get rid of the elastic augment reflection padding
  # and the invalid affinities (that's why we have additional lowe z crop)
  crop_after_target:
    crop_left: [5, 50, 50]
    crop_right: [0, 50, 50]
  random_slides: False
#  shape_after_slide: [478, 478]

offsets: [[-1, 0, 0], [0, -1, 0], [0, 0, -1], [-2, 0, 0], [0, -3, 0], [0, 0, -3], [-3, 0, 0],
                    [0, -9, 0], [0, 0, -9], [-4, 0, 0], [0, -27, 0], [0, 0, -27]]
    
# Specify configuration for the loader
loader_config:
  # Number of processes to use for loading data. Set to (say) 10 if you wish to
  # use 10 CPU cores, or to 0 if you wish to use the same process for training and
  # data-loading (generally not recommended).
  batch_size: 2
  num_workers: 40
  drop_last: True
  pin_memory: True
  shuffle: True
