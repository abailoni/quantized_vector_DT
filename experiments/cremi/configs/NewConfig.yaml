shortcuts:
  out_channels: &out_channels 56 # (n_classes*2-1)*n_directions
  xy_directions: &n_directions 8

device: cuda

loaders:
  general:
    volume_config:
      segmentation:
        affinity_config:
          retain_mask: True # This keep a mask of the valid affinities (not involving the ignore-label)
          retain_segmentation: False # This keeps the label image in the inputs
          ignore_label: 0

    defect_augmentation_config:
        p_missing_slice: 0.03
        p_low_contrast: 0.03
        p_deformed_slice: 0.03
        p_artifact_source: 0.03
        deformation_mode: 'compress'
        deformation_strength: 16
        artifact_source:
            min_masking_ratio: .5
            slicing_config:
              window_size: [1, 270, 270]  #change x and y to window size
              stride: [1, 128, 128]
              downsampling_ratio: [1, 1, 1]
            volume_config:
              artifacts:
                path: '/export/home/abailoni/datasets/cremi/constantin_data/sample_ABC_padded_20160501.defects.hdf'
                path_in_h5_dataset: 'defect_sections/raw'
                dtype: float32
              alpha_mask:
                path: '/export/home/abailoni/datasets/cremi/constantin_data/sample_ABC_padded_20160501.defects.hdf'
                path_in_h5_dataset: 'defect_sections/mask'
            master_config:
              elastic_transform:
                alpha: 2000.
                sigma: 50.

    # Configuration for the master dataset.
    master_config:
      # We might need order 0 interpolation if we have segmentation in there somewhere.
      - !Obj:LabelToDirections
        import_from: quantizedVDT.transforms
        n_directions: *n_directions
        compute_z:  &z_direction False

      - !Obj:Clip
        import_from: quantizedVDT.transforms
        a_max: &a_max 100
        apply_to: [1]


      - !Obj:Multiply
        import_from: quantizedVDT.transforms
        factor: *a_max
        invert_factor: True
        apply_to: [1]

      - !Obj:HomogenousQuantization
        import_from: quantizedVDT.transforms
        n_classes: 4
        max_distance: 1
        one_hot: False

    #      elastic_transform:
#        apply: False
#        alpha: 2000.
#        sigma: 50.
#        order: 0
#      random_slides: False
#      shape_after_slide: [243, 243]   #change to x, y of window size
#
#      random_flip: False
#
#      compute_directions:
#        n_directions: *n_directions
#        z_direction:  &z_direction False
#
#      clip:
#        a_max: &a_max 100
#
#      multiply:
#        factor: *a_max
#        invert_factor: True

    # Specify configuration for the loader
    loader_config:
      # Number of processes to use for loading data. Set to (say) 10 if you wish to
      # use 10 CPU cores, or to 0 if you wish to use the same process for training and
      # data-loading (generally not recommended).
      batch_size: 1
      num_workers: 6
      drop_last: True
      pin_memory: False
      shuffle: True



  train:
    names:
      - A
      - B
      - C

    # Specify how the data needs to be sliced before feeding to the network.
    # We use a 3D sliding window over the dataset to extract patches, which
    # are then fed to the network as batches.
    slicing_config:
      # Sliding window size
      window_size:
        A: &shape [3, 270, 270]
        B: *shape
        C: *shape
      # Sliding window stride
      stride:
        A: [4, 128, 128]
        B: [4, 128, 128]
        C: [4, 128, 128]
      # Data slice to iterate over.
      data_slice:
        A: ':, :, :'
        B: ':, :, :'
        C: ':75, :, :'

    # Specify paths to volumes
    volume_config:
      # Raw data
      raw:
        path:
          A: '/export/home/abailoni/datasets/cremi/SOA_affinities/sampleA_train.h5'
          B: '/export/home/abailoni/datasets/cremi/SOA_affinities/sampleB_train.h5'
          C: '/export/home/abailoni/datasets/cremi/SOA_affinities/sampleC_train.h5'
        path_in_file:
          A: 'raw'
          B: 'raw'
          C: 'raw'
        dtype: float32
        sigma: 0.025
      # Segmentation
      segmentation:
        path:
          A: '/export/home/abailoni/datasets/cremi/SOA_affinities/sampleA_train.h5'
          B: '/export/home/abailoni/datasets/cremi/SOA_affinities/sampleB_train.h5'
          C: '/export/home/abailoni/datasets/cremi/SOA_affinities/sampleC_train.h5'
        path_in_file:
          A: 'segmentations/groundtruth_fixed'
          B: 'segmentations/groundtruth_fixed'
          C: 'segmentations/groundtruth_fixed'
        dtype: float32



  val:
    names:
      - C

    slicing_config:
      window_size:
        C: *shape
      stride:
        C: [4, 128, 128]
      data_slice:
        C: '75:, :, :'

    volume_config:
      raw:
        path:
          C: '/export/home/abailoni/datasets/cremi/SOA_affinities/sampleC_train.h5'
        path_in_file:
          C: 'raw'
        dtype: float32
        sigma: 0.025
      segmentation:
        affinity_config:
          retain_segmentation: True # This keeps the label image in the inputs
        path:
          C: '/export/home/abailoni/datasets/cremi/SOA_affinities/sampleC_train.h5'
        path_in_file:
          C: 'segmentations/groundtruth_fixed'
        dtype: float32

model:
  UNet3D:
    import_from: embeddingutils.models.unet
    conv_type': ConvELU3D
#    loadfrom: '/export/home/claun/PycharmProjects/quantized_vector_DT/runs/cremi/speedrun/quantized_copy/checkpoint.pytorch'
    in_channels: 1
    fmaps: [80, 160, 320, 640] #[80, 160, 320, 640]
    out_channels: *out_channels
    depth: 3
    upsampling_mode: 'nearest'
    scale_factor:
      - [1, 3, 3]
      - [1, 3, 3]
      - [3, 3, 3]
    #final_activation: 'ReLU'
    final_activation: !Obj:Conv3D
      import_from: inferno.extensions.layers.convolutional
      in_channels: *out_channels
      out_channels: *out_channels
      kernel_size: 3
      activation: !Obj:PartialSoftmax
        import_from: torch.nn
        dim: -4
        nclasses: 4
        ndirs: *n_directions


trainer:
  max_epochs: 10 # not basically infinite
#  TODO: increase this if we need both affinity targets and directional DT targets
  num_targets: 1

  criterion:
    # TODO: here you will be able to add the final losses (SoresenDice for affinities and L1 for DT)
    losses:
#      loss: !Obj:L1andCEloss
      loss: !Obj:L1andSDloss
        import_from: quantizedVDT.losses
        n_channels: 4
        n_directions: 8
        weights: [1., 10.]
        exclude_borders: [1, 32, 32]


  metric:
    evaluate_every: 10
    quantizedVDT.metrics.L1fromQuantized:
      n_classes: 4
      max_distance: 1.
      n_distances: 8

  optimizer:
    Adam:
      lr: 1.0e-4
      weight_decay: 0.0005
  #      betas: [0.9, 0.999]

  intervals:
    save_every: [1000, 'iterations']
    validate_every:
      frequency: [200, 'iterations']
      for_num_iterations: 10

  tensorboard:
    log_scalars_every: [1, 'iterations']
    log_images_every: [100, 'iterations']
    log_histograms_every: 'never'


  callbacks:
#    gradients:
#      LogOutputGradients:
#        frequency: 1
#    quantizedVDT.callbacks:
#      SaveModelCallback:
#        save_every: 10
    essentials:
      SaveAtBestValidationScore:
        smoothness: 0
        verbose: True
#      GradientClip:
#        clip_value: 1e-3

    scheduling:
      AutoLR:
        monitor: 'validation_loss'
        factor: 0.99
        patience: '100 iterations'
        monitor_while: 'validating'
        monitor_momentum: 0.75
#        cooldown_duration: '50000 iterations'
        consider_improvement_with_respect_to: 'previous'
        verbose: True

#firelight:
#  VAE:
#    ImageGridVisualizer:
#
#      input_mapping:
##        global: [B: 0] # the mapping specified in 'global' is applied to all keys
#        input: ['inputs', index: 0, B: 0, D: 0]
#        target: ['target', index: 0, B: 0, D: 0]
#        preds: ['prediction', index: 0, B: 0, D: 0]
#
#      pad_width: 1  # width of the border between images in pixels
#      pad_value: .2  # intensity of the border pixels
#      upsampling_factor: 3  # the whole grid is upsampled by this factor
#
#      row_specs: ['H', 'S', 'D', 'V']
#      column_specs: ['W', 'C', 'B']
#
#      # Container visualizers always have the 'visualizers' argument. Its value has to be a list of visualizers
#      visualizers:
#
#        # visualize raw input
#        - IdentityVisualizer:
#            input: 'preds'
#            cmap: gray
#
#        - IdentityVisualizer:
#            input: 'target'
#            cmap: gray